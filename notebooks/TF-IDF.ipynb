{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/miboj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/miboj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import heapq\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 92579\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        # load document\n",
    "        doc = load_doc(filename)\n",
    "\n",
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "    # find first highlight\n",
    "    index = doc.find('@highlight')\n",
    "    # split into story and highlights\n",
    "    story, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "    # strip extra white space around each highlight\n",
    "    highlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "    return story, highlights\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "    all_stories = list()\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        # load document\n",
    "        doc = load_doc(filename)\n",
    "        # split into story and highlights\n",
    "        story, highlights = split_story(doc)\n",
    "        # store\n",
    "        all_stories.append({'story':story, 'highlights':highlights})\n",
    "    return all_stories\n",
    "\n",
    "# load stories\n",
    "p = Path(os.getcwd()).parents[0]\n",
    "cnn_path = str(p) + r'/data/external/cnn/stories'\n",
    "stories = load_stories(cnn_path)\n",
    "print('Loaded Stories %d' % len(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_marks(text) :\n",
    "    punctuation_marks = dict((ord(punctuation_mark), None) for punctuation_mark in string.punctuation)\n",
    "    return text.translate(punctuation_marks)\n",
    "\n",
    "def get_tokens(text) :\n",
    "    normalized_tokens = nltk.word_tokenize(remove_punctuation_marks(text.lower()))\n",
    "    # Lemmatized\n",
    "    #return [nltk.stem.WordNetLemmatizer().lemmatize(normalized_token) for normalized_token in normalized_tokens]\n",
    "    # Stemmed\n",
    "    return [nltk.stem.PorterStemmer().stem(normalized_token) for normalized_token in normalized_tokens]\n",
    "\n",
    "def calculate_sentence_scores(sentence_tokens, tfIdf):\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        #for word in nltk.word_tokenize(sent.lower()):\n",
    "        for word in get_tokens(sent):    \n",
    "            if word in tfIdf.keys():\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = tfIdf[word]/len(sent)\n",
    "                else:\n",
    "                    sentence_scores[sent] += tfIdf[word]/len(sent)                    \n",
    "    return sentence_scores\n",
    "\n",
    "def get_summary(summary_max_length, sentence_scores):\n",
    "    summary_sentences = heapq.nlargest(summary_max_length, sentence_scores, key=sentence_scores.get)\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X = [i['story'] for i in stories]\n",
    "y = [i['highlights'] for i in stories]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74063\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miboj/anaconda3/envs/NLP-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hi', 'isnt', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'thi', 'veri', 'wa', 'wasnt', 'werent', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "stpwrds = stopwords.words('english') + list(string.punctuation) + ['—', '“', '”', \"'\", \"’\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer = get_tokens, stop_words = stpwrds)\n",
    "tfIdf = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dhabi</th>\n",
       "      <td>0.327725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abu</th>\n",
       "      <td>0.299619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newsgath</th>\n",
       "      <td>0.293579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>middl</th>\n",
       "      <td>0.250523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>east</th>\n",
       "      <td>0.250408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maddox</th>\n",
       "      <td>0.210150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.209359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prism</th>\n",
       "      <td>0.155704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <td>0.142388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stan</th>\n",
       "      <td>0.127491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>0.116084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>broadcast</th>\n",
       "      <td>0.112263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>0.112093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>show</th>\n",
       "      <td>0.111819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>launch</th>\n",
       "      <td>0.111205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daili</th>\n",
       "      <td>0.101537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intern</th>\n",
       "      <td>0.096925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>0.089002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uae</th>\n",
       "      <td>0.086676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>0.085307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TF-IDF\n",
       "dhabi      0.327725\n",
       "abu        0.299619\n",
       "newsgath   0.293579\n",
       "middl      0.250523\n",
       "east       0.250408\n",
       "maddox     0.210150\n",
       "cnn        0.209359\n",
       "prism      0.155704\n",
       "region     0.142388\n",
       "stan       0.127491\n",
       "news       0.116084\n",
       "broadcast  0.112263\n",
       "new        0.112093\n",
       "show       0.111819\n",
       "launch     0.111205\n",
       "daili      0.101537\n",
       "intern     0.096925\n",
       "product    0.089002\n",
       "uae        0.086676\n",
       "content    0.085307"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play']\n",
      "playing\n",
      "play\n",
      "play\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "# Testing cell\n",
    "a = get_tokens('playing')\n",
    "b = nltk.stem.WordNetLemmatizer().lemmatize('playing')\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "print(a)\n",
    "print(b)\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "lancaster = nltk.stem.LancasterStemmer()\n",
    "print(porter.stem('playing'))\n",
    "print(lancaster.stem('playing'))\n",
    "print(wnl.lemmatize('playing', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONDON, England (CNN) -- If your neighbor mentions their green roof you might think they have a moss problem. \"In Switzerland, green roofs are federal law, but again this is interpreted at a cantonal and city level,\" he told CNN. \"In Germany they are down to $20 per square meter, which is way cheaper than a regular roof here,\" she told CNN.\n"
     ]
    }
   ],
   "source": [
    "# Document to summarized\n",
    "document = nltk.sent_tokenize(stories[1]['story'])\n",
    "#document = stories[1]['story']\n",
    "\n",
    "tfIdf_dict = df.to_dict()\n",
    "sentence_scores = calculate_sentence_scores(document, tfIdf_dict['TF-IDF'])\n",
    "\n",
    "summary = get_summary(3, sentence_scores)\n",
    "print(summary)\n",
    "#print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'f': 0.1308411166215392, 'p': 0.11290322580645161, 'r': 0.15555555555555556}, 'rouge-2': {'f': 0.019047614178686056, 'p': 0.01639344262295082, 'r': 0.022727272727272728}, 'rouge-l': {'f': 0.10869564725897944, 'p': 0.09615384615384616, 'r': 0.125}}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(summary, ''.join(stories[1]['highlights']))\n",
    "#scores1 = rouge.get_scores(''.join(stories[1]['highlights']), ''.join(stories[1]['highlights']))\n",
    "print(scores)\n",
    "print()\n",
    "#print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP-env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
