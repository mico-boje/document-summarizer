{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/miboj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/miboj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 92579\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        # load document\n",
    "        doc = load_doc(filename)\n",
    "\n",
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "    # find first highlight\n",
    "    index = doc.find('@highlight')\n",
    "    # split into story and highlights\n",
    "    story, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "    # strip extra white space around each highlight\n",
    "    highlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "    return story, highlights\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "    all_stories = list()\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        # load document\n",
    "        doc = load_doc(filename)\n",
    "        # split into story and highlights\n",
    "        story, highlights = split_story(doc)\n",
    "        # store\n",
    "        all_stories.append({'story':story, 'highlights':highlights})\n",
    "    return all_stories\n",
    "\n",
    "# load stories\n",
    "p = Path(os.getcwd()).parents[0]\n",
    "cnn_path = str(p) + r'/data/external/cnn/stories'\n",
    "stories = load_stories(cnn_path)\n",
    "print('Loaded Stories %d' % len(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "HANDICAP = 1\n",
    "\n",
    "def remove_punctuation_marks(text) :\n",
    "    punctuation_marks = dict((ord(punctuation_mark), None) for punctuation_mark in string.punctuation)\n",
    "    return text.translate(punctuation_marks)\n",
    "\n",
    "def get_lemmatized_tokens(text) :\n",
    "    normalized_tokens = nltk.word_tokenize(remove_punctuation_marks(text.lower()))\n",
    "    return [nltk.stem.WordNetLemmatizer().lemmatize(normalized_token) for normalized_token in normalized_tokens]\n",
    "\n",
    "def get_average(values) :\n",
    "    greater_than_zero_count = total = 0\n",
    "    for value in values :\n",
    "        if value != 0 :\n",
    "            greater_than_zero_count += 1\n",
    "            total += value \n",
    "    return total / greater_than_zero_count\n",
    "\n",
    "def get_threshold(tfidf_results) :\n",
    "    i = total = 0\n",
    "    while i < (tfidf_results.shape[0]) :\n",
    "        total += get_average(tfidf_results[i, :].toarray()[0])\n",
    "        i += 1\n",
    "    return total / tfidf_results.shape[0]\n",
    "\n",
    "def get_summary(documents, tfidf_results) :\n",
    "    summary = \"\"\n",
    "    i = 0\n",
    "    while i < (tfidf_results.shape[0]) :\n",
    "        if (get_average(tfidf_results[i, :].toarray()[0])) >= get_threshold(tfidf_results) * HANDICAP :\n",
    "                summary += ' ' + documents[i]\n",
    "        i += 1\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miboj/anaconda3/envs/NLP-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arent', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'ha', 'hadnt', 'hasnt', 'havent', 'isnt', 'mightnt', 'mustnt', 'neednt', 'shant', 'shes', 'shouldnt', 'shouldve', 'thatll', 'wa', 'wasnt', 'werent', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "documents = nltk.sent_tokenize(stories[1]['story'])\n",
    "tfidf_results = TfidfVectorizer(tokenizer = get_lemmatized_tokens, \n",
    "                                stop_words = stopwords.words('english')).fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Maybe they are simply referring to the color. But you're unlikely to think that they have just had a mini ecosystem installed. But green roofs are not just aesthetic. The industry is not faring so well in other parts of the world. In North America, green roofs have taken even longer to catch on. But this was up 80 percent from the previous year, and the market continues to grow. \"There are mandates over there because of the storm water they retain,\" she continued, \"Which is a huge drain on their resources, as it is on ours. What we are trying to do is champion the policies behind storm water.\" Storm water is a growing problem in cities. \"This will encourage uptake.\" Despite this lack of support, the green roof industry is growing. Cities such as London and Sheffield are now asking for them as part of planning applications.\" \"This is a much better approach than driven by the center.\" In an industry that varies from project to project, this flexibility is a valuable asset. But then, it's not easy being green. ...................................\n",
      "\n",
      "Would you put a green roof on your home? Do green roofs really help the environment -- or are they an expensive indulgence? Share your views in the Sound Off box below.\n"
     ]
    }
   ],
   "source": [
    "summary = get_summary(documents, tfidf_results)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'f': 0.1439393911113981, 'p': 0.0867579908675799, 'r': 0.4222222222222222}, 'rouge-2': {'f': 0.015267172777810662, 'p': 0.009174311926605505, 'r': 0.045454545454545456}, 'rouge-l': {'f': 0.12087911744958348, 'p': 0.07746478873239436, 'r': 0.275}}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(summary, ''.join(stories[1]['highlights']))\n",
    "#scores1 = rouge.get_scores(''.join(stories[1]['highlights']), ''.join(stories[1]['highlights']))\n",
    "print(scores)\n",
    "print()\n",
    "#print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP-env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
