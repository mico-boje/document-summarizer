{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/miboj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/miboj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import heapq\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/miboj/NLP/document-summarizer/data/external/cnn/stories'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f492c84fb630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mcnn_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mr'/data/external/cnn/stories'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mstories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_stories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded Stories %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f492c84fb630>\u001b[0m in \u001b[0;36mload_stories\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_stories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mall_stories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# load document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/miboj/NLP/document-summarizer/data/external/cnn/stories'"
     ]
    }
   ],
   "source": [
    "filename = r'/home/miboj/NLP/document-summarizer/data/processed/articles.json'\n",
    "file = open(filename, encoding='ascii', errors='ignore')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "d = ast.literal_eval(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "filename = r'/home/miboj/NLP/document-summarizer/data/raw/articles.json'\n",
    "file = open(filename, encoding='ascii', errors='ignore')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "json_content = ast.literal_eval(text)\n",
    "samples = json_content[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "sentences = []\n",
    "word_count = 0\n",
    "stpwrds = stopwords.words('english') + list(string.punctuation) + ['—', '“', '”', \"'\", \"’\"]\n",
    "for e, i in enumerate(tokens_list):\n",
    "    words = []\n",
    "    a = nltk.word_tokenize(i)\n",
    "    for word in a:\n",
    "        if word not in stpwrds:\n",
    "            words.append(word)\n",
    "            word_count += 1\n",
    "    sentences.append(words)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_marks(text) :\n",
    "    punctuation_marks = dict((ord(punctuation_mark), None) for punctuation_mark in string.punctuation)\n",
    "    return text.translate(punctuation_marks)\n",
    "\n",
    "def get_tokens(text) :\n",
    "    normalized_tokens = nltk.word_tokenize(remove_punctuation_marks(text.lower()))\n",
    "    # Lemmatized\n",
    "    #return [nltk.stem.WordNetLemmatizer().lemmatize(normalized_token) for normalized_token in normalized_tokens]\n",
    "    # Stemmed\n",
    "    return [nltk.stem.PorterStemmer().stem(normalized_token) for normalized_token in normalized_tokens]\n",
    "\n",
    "def calculate_sentence_scores(sentence_tokens, tfIdf):\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        #for word in nltk.word_tokenize(sent.lower()):\n",
    "        for word in get_tokens(sent):    \n",
    "            if word in tfIdf.keys():\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = tfIdf[word]/len(sent)\n",
    "                else:\n",
    "                    sentence_scores[sent] += tfIdf[word]/len(sent)                    \n",
    "    return sentence_scores\n",
    "\n",
    "def get_summary(summary_max_length, sentence_scores):\n",
    "    summary_sentences = heapq.nlargest(summary_max_length, sentence_scores, key=sentence_scores.get)\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-71c4d21773fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstpwrds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtfIdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "stpwrds = stopwords.words('english') + list(string.punctuation) + ['—', '“', '”', \"'\", \"’\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer = get_tokens, stop_words = stpwrds)\n",
    "tfIdf = vectorizer.fit_transform(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfIdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-17fe7fe41a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfIdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TF-IDF\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TF-IDF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfIdf' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play']\n",
      "playing\n",
      "play\n",
      "play\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "# Testing cell\n",
    "a = get_tokens('playing')\n",
    "b = nltk.stem.WordNetLemmatizer().lemmatize('playing')\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "print(a)\n",
    "print(b)\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "lancaster = nltk.stem.LancasterStemmer()\n",
    "print(porter.stem('playing'))\n",
    "print(lancaster.stem('playing'))\n",
    "print(wnl.lemmatize('playing', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_empty_string(input_string):\n",
    "    for e, i in enumerate(input_string):\n",
    "        if i[-1] == ' ' and input_string[e+1][-1] == ' ':\n",
    "            input_string[e] = i.rstrip()\n",
    "    joined_string = ''.join(input_string)\n",
    "    for e, i in enumerate(joined_string):\n",
    "        if i == ' ' and joined_string[e+1] == ' ':\n",
    "            del i\n",
    "    sentences = nltk.sent_tokenize(joined_string)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ROME — Defying reports that their planned partnership is doomed to fail, France’s Naval Group and Italy’s Fincantieri have announced a joint venture to build and export naval vessels.', 'The two state-controlled shipyards said they were forming a 50-50 joint venture after months of talks to integrate their activities.', 'The move comes as Europe’s fractured shipbuilding industry faces stiffer global competition.', 'The firms said in a statement that the deal would allow them to “jointly prepare winning offers for binational programs and export market,” as well as create joint supply chains, research and testing.', 'Naval Group and Fincantieri first announced talks on cooperation last year after the latter negotiated a controlling share in French shipyard STX.', 'But the deal was reportedly losing momentum due to resistance from French industry and a political row between France and Italy over migrants.', 'The new deal falls short of the 10 percent share swap predicted by French Economy and Finance Minister Bruno Le Maire earlier this year, and far short of the total integration envisaged by Fincantieri CEO Giuseppe Bono.', \"The statement called the joint venture the “first steps” toward the creation of an alliance that would create “a more efficient and competitive European shipbuilding industry.” Naval Group CEO Hervé Guillou, speaking at the Euronaval trade expo in Paris on Oct. 24, said the alliance is based on “two countries sharing a veritable naval ambition.” The joint venture is necessary because the “context of the global market has changed drastically,” he added, specifically mentioning new market entrants Russia, China, Singapore, Ukraine, India and Turkey.Sign up for the Early Bird Brief, the defense industry's most comprehensive news and information, straight to your inbox.By giving us your email, you are opting in to the Early Bird Brief.\", 'When asked about an initial product to be tackled under the alliance, Guillou acknowledged: “The answer is simple: there is nothing yet.” However, the firms said they are working toward a deal to build four logistics support ships for the French Navy, which will be based on an Italian design.', 'Competition flares up for the follow-on portion of a deal previously won by the French shipbuilder.', 'The firms also plan to jointly bid next year on work for midlife upgrades for Horizon frigates, which were built by France and Italy and are in service with both navies.', 'The work would include providing a common combat management system.', 'The statement was cautious about future acceleration toward integration.', '“A Government-to-Government Agreement would be needed to ensure the protection of sovereign assets, a fluid collaboration between the French and Italian teams and encourage further coherence of the National assistance programs, which provide a framework and support export sales,” the statement said.', 'But the firms were optimistic the deal would be “a great opportunity for both groups and their eco-systems, by enhancing their ability to better serve the Italian and French navies, to capture new export contracts, to increase research funding and, ultimately, improve the competitiveness of both French and Italian naval sectors.” Sebastian Sprenger in Paris contributed to this report.']\n"
     ]
    }
   ],
   "source": [
    "# Document to summarized\n",
    "#document = nltk.sent_tokenize(stories[1]['story'])\n",
    "raw_string = [\" ROME — Defying reports that their planned partnership is \", \"doomed to fail\", \", France’s Naval Group and \", \"Italy’s Fincantieri\", \" have announced a joint venture to build and export naval vessels. \", \" The two \", \"state-controlled shipyards\", \" said they were forming a 50-50 joint venture after months of talks to integrate their activities. The move comes as Europe’s fractured shipbuilding industry faces stiffer global competition. \", \" The firms said in a statement that the deal would allow them to “jointly prepare winning offers for binational programs and export market,” as well as create joint supply chains, research and testing. \", \" Naval Group and Fincantieri first announced talks on cooperation last year after the latter negotiated a controlling share in French shipyard STX. But the deal was reportedly losing momentum due to resistance from French industry and a political row between France and Italy over migrants. \", \" The new deal falls short of the 10 percent share swap predicted by French Economy and Finance Minister Bruno Le Maire earlier this year, and far short of the total integration envisaged by Fincantieri CEO Giuseppe Bono. \", \" The statement called the joint venture the “first steps” toward the creation of an alliance that would create “a more efficient and competitive European shipbuilding industry.”\", \" Naval Group CEO Hervé Guillou, speaking at the Euronaval trade expo in Paris on Oct. 24, said the alliance is based on “two countries sharing a veritable naval ambition.”\", \" The joint venture is necessary because the “context of the global market has changed drastically,” he added, specifically mentioning new market entrants Russia, China, Singapore, Ukraine, India and Turkey.\", \"Sign up for the Early Bird Brief, the defense industry's most comprehensive news and information, straight to your inbox.\", \"By giving us your email, you are opting in to the Early Bird Brief.\", \" When asked about an initial product to be tackled under the alliance, Guillou acknowledged: “The answer is simple: there is nothing yet.”\", \" However, the firms said they are working toward a deal to build four logistics support ships for the French Navy, which will be based on an Italian design. \", \"Competition flares up for the follow-on portion of a deal previously won by the French shipbuilder.\", \" The firms also plan to jointly bid next year on work for midlife upgrades for Horizon frigates, which were built by France and Italy and are in service with both navies. The work would include providing a common combat management system. \", \" The statement was cautious about future acceleration toward integration. “A Government-to-Government Agreement would be needed to ensure the protection of sovereign assets, a fluid collaboration between the French and Italian teams and encourage further coherence of the National assistance programs, which provide a framework and support export sales,” the statement said.\", \" But the firms were optimistic the deal would be “a great opportunity for both groups and their eco-systems, by enhancing their ability to better serve the Italian and French navies, to capture new export contracts, to increase research funding and, ultimately, improve the competitiveness of both French and Italian naval sectors.”\", \" \", \"Sebastian Sprenger\", \" in Paris contributed to this report.\"]\n",
    "document = remove_empty_string(raw_string)\n",
    "\n",
    "#document = stories[1]['story']\n",
    "\n",
    "tfIdf_dict = df.to_dict()\n",
    "sentence_scores = calculate_sentence_scores(document, tfIdf_dict['TF-IDF'])\n",
    "\n",
    "summary = get_summary(3, sentence_scores)\n",
    "print(summary)\n",
    "#print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'f': 0.1308411166215392, 'p': 0.11290322580645161, 'r': 0.15555555555555556}, 'rouge-2': {'f': 0.019047614178686056, 'p': 0.01639344262295082, 'r': 0.022727272727272728}, 'rouge-l': {'f': 0.10869564725897944, 'p': 0.09615384615384616, 'r': 0.125}}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(summary, ''.join(stories[1]['highlights']))\n",
    "#scores1 = rouge.get_scores(''.join(stories[1]['highlights']), ''.join(stories[1]['highlights']))\n",
    "print(scores)\n",
    "print()\n",
    "#print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
