{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/miboj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/miboj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import heapq\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 92579\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        # load document\n",
    "        doc = load_doc(filename)\n",
    "\n",
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "    # find first highlight\n",
    "    index = doc.find('@highlight')\n",
    "    # split into story and highlights\n",
    "    story, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "    # strip extra white space around each highlight\n",
    "    highlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "    return story, highlights\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "    all_stories = list()\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        # load document\n",
    "        doc = load_doc(filename)\n",
    "        # split into story and highlights\n",
    "        story, highlights = split_story(doc)\n",
    "        # store\n",
    "        all_stories.append({'story':story, 'highlights':highlights})\n",
    "    return all_stories\n",
    "\n",
    "# load stories\n",
    "p = Path(os.getcwd()).parents[0]\n",
    "cnn_path = str(p) + r'/data/external/cnn/stories'\n",
    "stories = load_stories(cnn_path)\n",
    "print('Loaded Stories %d' % len(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_marks(text) :\n",
    "    punctuation_marks = dict((ord(punctuation_mark), None) for punctuation_mark in string.punctuation)\n",
    "    return text.translate(punctuation_marks)\n",
    "\n",
    "def get_tokens(text) :\n",
    "    normalized_tokens = nltk.word_tokenize(remove_punctuation_marks(text.lower()))\n",
    "    # Lemmatized\n",
    "    #return [nltk.stem.WordNetLemmatizer().lemmatize(normalized_token) for normalized_token in normalized_tokens]\n",
    "    # Stemmed\n",
    "    return [nltk.stem.PorterStemmer().stem(normalized_token) for normalized_token in normalized_tokens]\n",
    "\n",
    "def calculate_sentence_scores(sentence_tokens, tfIdf):\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        #for word in nltk.word_tokenize(sent.lower()):\n",
    "        for word in get_tokens(sent):    \n",
    "            if word in tfIdf.keys():\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = tfIdf[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += tfIdf[word]                    \n",
    "    return sentence_scores\n",
    "\n",
    "def get_summary(summary_max_length, sentence_scores):\n",
    "    summary_sentences = heapq.nlargest(summary_max_length, sentence_scores, key=sentence_scores.get)\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X = [i['story'] for i in stories]\n",
    "y = [i['highlights'] for i in stories]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74063\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miboj/anaconda3/envs/NLP-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hi', 'isnt', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'thi', 'veri', 'wa', 'wasnt', 'werent', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "stpwrds = stopwords.words('english') + list(string.punctuation) + ['—', '“', '”', \"'\", \"’\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer = get_tokens, stop_words = stpwrds)\n",
    "tfIdf = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shokalskiy</th>\n",
       "      <td>0.360352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ice</th>\n",
       "      <td>0.353056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>icebreak</th>\n",
       "      <td>0.246709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>akademik</th>\n",
       "      <td>0.216211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ship</th>\n",
       "      <td>0.214875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polar</th>\n",
       "      <td>0.209014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guard</th>\n",
       "      <td>0.205191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coast</th>\n",
       "      <td>0.203746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xue</th>\n",
       "      <td>0.196144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>australian</th>\n",
       "      <td>0.168889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>akadmik</th>\n",
       "      <td>0.163726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcmurdo</th>\n",
       "      <td>0.153455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chines</th>\n",
       "      <td>0.124021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vessel</th>\n",
       "      <td>0.110957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>antarct</th>\n",
       "      <td>0.109729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>antarctica</th>\n",
       "      <td>0.107157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knot</th>\n",
       "      <td>0.095902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>star</th>\n",
       "      <td>0.095510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>russian</th>\n",
       "      <td>0.094897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passeng</th>\n",
       "      <td>0.091654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              TF-IDF\n",
       "shokalskiy  0.360352\n",
       "ice         0.353056\n",
       "icebreak    0.246709\n",
       "akademik    0.216211\n",
       "ship        0.214875\n",
       "polar       0.209014\n",
       "guard       0.205191\n",
       "coast       0.203746\n",
       "xue         0.196144\n",
       "australian  0.168889\n",
       "akadmik     0.163726\n",
       "mcmurdo     0.153455\n",
       "chines      0.124021\n",
       "vessel      0.110957\n",
       "antarct     0.109729\n",
       "antarctica  0.107157\n",
       "knot        0.095902\n",
       "star        0.095510\n",
       "russian     0.094897\n",
       "passeng     0.091654"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing cell\n",
    "a = get_lemmatized_tokens('playing')\n",
    "b = nltk.stem.WordNetLemmatizer().lemmatize('playing')\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "print(a)\n",
    "print(b)\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "lancaster = nltk.stem.LancasterStemmer()\n",
    "print(porter.stem('playing'))\n",
    "print(lancaster.stem('playing'))\n",
    "print(wnl.lemmatize('playing', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majora Carter, who set up Sustainable South Bronx to help lift the area out of poverty by creating green-collar jobs, is frustrated by this difference between the European and American industries. Dusty Gedge, co-founder of Livingroof.org, a UK Web site promoting the green roof industry, believes it is the government's responsibility to help the industry grow. Do green roofs really help the environment -- or are they an expensive indulgence?\n"
     ]
    }
   ],
   "source": [
    "# Document to summarized\n",
    "document = nltk.sent_tokenize(stories[1]['story'])\n",
    "#document = stories[1]['story']\n",
    "\n",
    "tfIdf_dict = df.to_dict()\n",
    "sentence_scores = calculate_sentence_scores(document, tfIdf_dict['TF-IDF'])\n",
    "\n",
    "summary = get_summary(3, sentence_scores)\n",
    "print(summary)\n",
    "#print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(summary, ''.join(stories[1]['highlights']))\n",
    "#scores1 = rouge.get_scores(''.join(stories[1]['highlights']), ''.join(stories[1]['highlights']))\n",
    "print(scores)\n",
    "print()\n",
    "#print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP-env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
